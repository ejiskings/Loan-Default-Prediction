{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV,StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    mean_squared_error\n",
    ")\n",
    "import joblib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import pandas as pd\n",
    "\n",
    "def train_test_split_data(df, target_col, test_size=0.2, random_state=84):\n",
    "    if target_col not in df.columns:\n",
    "        raise ValueError(f\"Target column '{target_col}' not found in the dataset.\")\n",
    "    \n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y if y.nunique() > 1 else None\n",
    "    )\n",
    "    \n",
    "    # Apply Random OverSampling on training data\n",
    "    over_sampler = RandomOverSampler(random_state=random_state)\n",
    "    X_train_resampled, y_train_resampled = over_sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Convert back to DataFrame for display\n",
    "    X_train_df = pd.DataFrame(X_train_resampled, columns=X.columns)\n",
    "\n",
    "    print(\"______MODEL INPUT FORMAT (1 sample)________\")\n",
    "    print(X_train_df.iloc[0].to_dict())  # As dictionary input to model\n",
    "    # Optional: if you want it as a row DataFrame\n",
    "    # print(X_train_df.iloc[[0]])\n",
    "\n",
    "    return X_train_resampled, X_test, y_train_resampled, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(df, cat_cols, encoder_path=\"encoders.pkl\"):\n",
    "    df_copy = df.copy()\n",
    "    le_dict = {}\n",
    "    \n",
    "    for col in cat_cols:\n",
    "        if col in df_copy.columns:\n",
    "            le = LabelEncoder()\n",
    "            df_copy[col] = le.fit_transform(df_copy[col].astype(str))\n",
    "            le_dict[col] = le\n",
    "        else:\n",
    "            print(f\"Warning: Column '{col}' not found in dataset, skipping encoding.\")\n",
    "    \n",
    "    # Save the encoders to a separate file\n",
    "    joblib.dump(le_dict, encoder_path)\n",
    "    \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, cat_cols=None, num_cols=None, encoder_path=\"encoders.pkl\", scaler_path=\"scaler.pkl\"):\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    if cat_cols:\n",
    "        df_copy = encode_data(df_copy, cat_cols, encoder_path)\n",
    "    \n",
    "    if num_cols:\n",
    "        scaler = StandardScaler()\n",
    "        df_copy[num_cols] = scaler.fit_transform(df_copy[num_cols])\n",
    "        \n",
    "        # Save the scaler to a separate file\n",
    "        joblib.dump(scaler, scaler_path)\n",
    "    \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Hyperparameter Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameters(model, param_grid, X_train, y_train, method=\"grid\"):\n",
    "    \n",
    "    if method == \"grid\":\n",
    "        search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "    elif method == \"random\":\n",
    "        search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=10, cv=5, random_state=42, n_jobs=-1, verbose=1)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be either 'grid' or 'random'.\")\n",
    "    \n",
    "    search.fit(X_train, y_train)\n",
    "    print(f\"Best Parameters for {model.__class__.__name__}: {search.best_params_}\")\n",
    "    \n",
    "    return search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluationg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def train_and_evaluate_models(X_train, y_train, X_test, y_test, model_save_path=\"best_model.pkl\"):\n",
    "    \n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000, class_weight=\"balanced\"),\n",
    "        'Decision Tree': DecisionTreeClassifier(random_state=42, class_weight=\"balanced\"),\n",
    "        'Random Forest': RandomForestClassifier(random_state=42, class_weight=\"balanced\"),\n",
    "        'Naive Bayes': GaussianNB(),\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    trained_models = {}\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        if model_name == \"Random Forest\":\n",
    "            param_grid = {'n_estimators': [100, 200, 400, 600], 'max_depth': [None, 10, 20, 40, 100], 'min_samples_split': [2, 5, 7, 10]}\n",
    "            best_model = tune_hyperparameters(model, param_grid, X_train, y_train, method=\"random\")\n",
    "        elif model_name == \"Decision Tree\":\n",
    "            param_grid = {'max_depth': [None, 10, 20, 40, 60, 100], 'min_samples_split': [2, 5, 7], 'criterion': ['gini', 'entropy']}\n",
    "            best_model = tune_hyperparameters(model, param_grid, X_train, y_train, method=\"grid\")\n",
    "        else:\n",
    "            best_model = model\n",
    "        \n",
    "        best_model.fit(X_train, y_train)\n",
    "        trained_models[model_name] = best_model\n",
    "        \n",
    "        y_pred = best_model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', zero_division=1)\n",
    "        \n",
    "        results.append({\n",
    "            'Model': model_name,\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1 Score': f1\n",
    "        })\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Identify the best model based on F1 Score\n",
    "    best_model_row = results_df.loc[results_df['Accuracy'].idxmax()]\n",
    "    best_model_name = best_model_row['Model']\n",
    "    best_model = trained_models[best_model_name]\n",
    "    \n",
    "    # Save the best model\n",
    "    try:\n",
    "        joblib.dump(best_model, model_save_path)\n",
    "        print(f\"Best model ({best_model_name}) saved to {model_save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving best model to {model_save_path}: {e}\")\n",
    "    \n",
    "    return results_df, trained_models, best_model_name, best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting confusion matrix, ROC curve, and MSE graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(models, X_test, y_test):\n",
    "    \"\"\"Plot model evaluation metrics attractively: Accuracy, Confusion Matrices, ROC Curve, and MSE.\"\"\"\n",
    "    \n",
    "    model_names = list(models.keys())\n",
    "    colors = ['#FF6F61', '#6B5B95', '#88B04B', '#F7CAC9']  # Custom colors\n",
    "\n",
    "    # 1. Compute all evaluation metrics\n",
    "    accuracies = []\n",
    "    mse_values = []\n",
    "    fpr_tpr_auc = {}  # Store ROC data  \n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i, (model_name, model) in enumerate(models.items()):\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else y_pred\n",
    "\n",
    "        # Compute metrics\n",
    "        accuracies.append(accuracy_score(y_test, y_pred))\n",
    "        mse_values.append(mean_squared_error(y_test, y_pred))\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "        auc_score = auc(fpr, tpr)\n",
    "        fpr_tpr_auc[model_name] = (fpr, tpr, auc_score)\n",
    "\n",
    "    # 2. Accuracy Bar Chart\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=model_names, y=accuracies, palette=colors)\n",
    "    plt.title(\"Model Accuracy Comparison\", fontsize=14, fontweight='bold')\n",
    "    plt.ylabel(\"Accuracy Score\")\n",
    "    plt.ylim(0, 1)\n",
    "    for i, acc in enumerate(accuracies):\n",
    "        plt.text(i, acc + 0.02, f\"{acc:.2f}\", ha='center', fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "    # 3. ROC Curves in One Graph\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i, (model_name, (fpr, tpr, auc_score)) in enumerate(fpr_tpr_auc.items()):\n",
    "        plt.plot(fpr, tpr, label=f\"{model_name} (AUC = {auc_score:.2f})\", color=colors[i])\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "    plt.title(\"ROC Curve Comparison\", fontsize=14, fontweight='bold')\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.legend(loc=\"lower right\", fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "    # 4. Confusion Matrices (2x2 Grid)\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    fig.suptitle(\"Confusion Matrices\", fontsize=16, fontweight='bold')\n",
    "\n",
    "    for i, (model_name, model) in enumerate(models.items()):\n",
    "        if i >= 4:\n",
    "            break\n",
    "        y_pred = model.predict(X_test)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        ax = axes[i // 2, i % 2]\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"coolwarm\", cbar=False, ax=ax, annot_kws={\"size\": 16})\n",
    "        ax.set_title(f\"{model_name}\", fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel(\"Predicted\", fontsize=10)\n",
    "        ax.set_ylabel(\"True\", fontsize=10)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "    # 5. MSE Comparison Bar Chart\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=model_names, y=mse_values, palette=colors)\n",
    "    plt.title(\"Mean Squared Error (MSE) Comparison\", fontsize=14, fontweight='bold')\n",
    "    plt.ylabel(\"MSE Value\")\n",
    "    for i, mse in enumerate(mse_values):\n",
    "        plt.text(i, mse + 0.02, f\"{mse:.2f}\", ha='center', fontsize=12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_performance_table(results):\n",
    "    \n",
    "    print(\"Performance Metrics:\")\n",
    "    print(results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"cleaned_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = df['loan_status'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def identify_column_types(df):\n",
    "\n",
    "    numerical_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "    categorical_cols = df.select_dtypes(exclude=['number']).columns.tolist()\n",
    "\n",
    "    return {\"0\": numerical_cols, \"1\": categorical_cols}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_types = identify_column_types(df)\n",
    "cat_cols = column_types['1']\n",
    "num_cols = column_types['0']\n",
    "target_col = \"loan_status\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = preprocess_data(df, cat_cols, num_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split_data(df_processed, target_col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df, trained_models, best_model_name, best_model = train_and_evaluate_models(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(trained_models, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_performance_table(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_feature_importances(trained_models, feature_names, top_n=10):\n",
    "    for model_name, model in trained_models.items():\n",
    "        # Tree-based models\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importances = model.feature_importances_\n",
    "            indices = np.argsort(importances)[-top_n:]\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.title(f'Top {top_n} Feature Importances - {model_name}')\n",
    "            plt.barh(range(len(indices)), importances[indices], align='center')\n",
    "            plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "            plt.xlabel('Importance Score')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        # Logistic Regression\n",
    "        elif model_name == \"Logistic Regression\" and hasattr(model, \"coef_\"):\n",
    "            coefs = model.coef_[0]\n",
    "            indices = np.argsort(np.abs(coefs))[-top_n:]\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.title(f'Top {top_n} Feature Influences - {model_name}')\n",
    "            plt.barh(range(top_n), coefs[indices], align='center')\n",
    "            plt.yticks(range(top_n), [feature_names[i] for i in indices])\n",
    "            plt.xlabel('Coefficient Value')\n",
    "            plt.axvline(0, color='grey', linestyle='--')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        # Naive Bayes\n",
    "        elif model_name == \"Naive Bayes\" and hasattr(model, \"feature_log_prob_\"):\n",
    "            log_probs = model.feature_log_prob_\n",
    "            mean_log_prob = np.mean(log_probs, axis=0)\n",
    "            indices = np.argsort(mean_log_prob)[-top_n:]\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.title(f'Top {top_n} Feature Influences - {model_name}')\n",
    "            plt.barh(range(top_n), mean_log_prob[indices], align='center')\n",
    "            plt.yticks(range(top_n), [feature_names[i] for i in indices])\n",
    "            plt.xlabel('Mean Log Probability')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        else:\n",
    "            print(f\"[Info] Feature importance/influence not available for model: {model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training\n",
    "results_df, trained_models = train_and_evaluate_models(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Feature importance plot\n",
    "plot_feature_importances(trained_models, feature_names=X_train.columns, top_n=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
